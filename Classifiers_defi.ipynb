{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00ca100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['aren', 'can', 'couldn', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'mustn', 're', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset for French:\n",
      "                           document  \\\n",
      "0  527ba1db806cfd245d90526c5a7b9342   \n",
      "1  150854b0beccb3ee7d55fad80f92e58b   \n",
      "2  e9ad60a84487a9e79cc3e95fc1d60f95   \n",
      "3  395be8ab43a1833438a972ac5ecd0f48   \n",
      "4  4722474dddcd6a982556ec48b8b07ab3   \n",
      "\n",
      "                                    word_occurrences catégorie  \n",
      "0  {'b': 0, 'london': 27, 'lion': 103, 'do': 44, ...         ?  \n",
      "1  {'b': 0, 'french': 110, 'football': 1626, 'pre...         ?  \n",
      "2  {'b': 0, 'patisser': 27, 'valer': 113, 'four':...         ?  \n",
      "3  {'b': 0, 'nas': 566, 'spacex': 232, 'launch': ...         ?  \n",
      "4  {'b': 0, 'russian': 17, 'influx': 1, 'driv': 3...         ?  \n",
      "\n",
      "Dataset for English:\n",
      "                           document  \\\n",
      "0  a5319b640496be985be9e46141a44d26   \n",
      "1  733691e2b8bb30046fcb3b3870acdda9   \n",
      "2  aae4c53ffd63ab90fafe911020a39411   \n",
      "3  bd90e1776424ffb662d7502846824612   \n",
      "4  729d9fa185f1ab06151e847d1e3aacea   \n",
      "\n",
      "                                    word_occurrences catégorie  \n",
      "0  {'b': 0, 'true': 155, 'coronavirus': 3840, 'de...         ?  \n",
      "1  {'b': 0, 'unearth': 42, 'maya': 28, 'civil': 2...         ?  \n",
      "2  {'b': 0, 't': 0, 'row': 361, 'price': 1403, 'r...         ?  \n",
      "3  {'b': 0, 'new': 7788, 'voluntari': 31, 'mobili...         ?  \n",
      "4  {'b': 0, 'bmw': 18, 'say': 9018, 'will': 7135,...         ?  \n"
     ]
    }
   ],
   "source": [
    "import shelve\n",
    "import snowballstemmer\n",
    "from stop_words import get_stop_words\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def separate_articles_by_language(article_db):\n",
    "    article_db_french = {}\n",
    "    article_db_english = {}\n",
    "\n",
    "    for article_id, article in article_db.items():\n",
    "        language = article.get('Langue', '').lower()\n",
    "        if language == 'fr':\n",
    "            article_db_french[article_id] = article\n",
    "        elif language == 'en':\n",
    "            article_db_english[article_id] = article\n",
    "\n",
    "    return article_db_french, article_db_english\n",
    "\n",
    "def process_articles(article_db, stopwords, stemmer):\n",
    "    all_processed_texts = []\n",
    "\n",
    "    for article_id, article in article_db.items():\n",
    "        title = article.get('Titre', '')\n",
    "        description = article.get('Description / Résumé', '')\n",
    "        content = article.get('Contenu', '')\n",
    "\n",
    "        # Combine the values and print\n",
    "        combined_text = f\"{title} {description} {content}\"\n",
    "\n",
    "        # Remove punctuation, make lowercase, and remove numbers\n",
    "        text_without_punctuation = re.sub(r'[^\\w\\s]', '', combined_text)\n",
    "        text_lower = text_without_punctuation.lower()\n",
    "        text_without_numbers = re.sub(r'\\d+', '', text_lower)\n",
    "\n",
    "        # Tokenize the text and perform stemming\n",
    "        words = text_without_numbers.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "        stemmed_words = [stemmer.stemWord(word) for word in filtered_words]\n",
    "\n",
    "        # Combine the stemmed words into a string for CountVectorizer\n",
    "        processed_text = ' '.join(stemmed_words)\n",
    "\n",
    "        # Append processed text to the list\n",
    "        all_processed_texts.append(processed_text)\n",
    "\n",
    "    return all_processed_texts\n",
    "\n",
    "def save_and_load_data(vectorizer, sparse_matrix, filename_feature, filename_matrix):\n",
    "    # Save feature names and sparse matrix into two files\n",
    "    joblib.dump(vectorizer.get_feature_names_out(), filename_feature)\n",
    "    joblib.dump(sparse_matrix, filename_matrix)\n",
    "\n",
    "    # Load feature names and sparse matrix back into memory\n",
    "    loaded_feature_names = joblib.load(filename_feature)\n",
    "    loaded_sparse_matrix = joblib.load(filename_matrix)\n",
    "\n",
    "    return loaded_feature_names, loaded_sparse_matrix\n",
    "\n",
    "def calculate_word_occurrences(feature_names, sparse_matrix):\n",
    "    total_word_occurrences = Counter()\n",
    "    for row in sparse_matrix:\n",
    "        total_word_occurrences += Counter({feature: count for feature, count in zip(feature_names, row.toarray()[0])})\n",
    "\n",
    "    return total_word_occurrences\n",
    "\n",
    "def display_word_occurrences(word_occurrences):\n",
    "    # Display the total occurrences of each word with more than 100 occurrences, sorted by count in descending order\n",
    "    print(\"\\nTotal Word Occurrences Across All Articles (with more than 100 occurrences), sorted by count:\")\n",
    "\n",
    "    # Sort the total_word_occurrences dictionary by count in descending order\n",
    "    sorted_word_occurrences = sorted(word_occurrences.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "    # Display sorted results\n",
    "    for word, count in sorted_word_occurrences:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "def create_dataset(article_db, stopwords, stemmer, vectorizer, filename_feature, filename_matrix, language):\n",
    "    all_processed_texts = process_articles(article_db, stopwords, stemmer)\n",
    "    all_sparse_matrix = vectorizer.transform(all_processed_texts)\n",
    "    loaded_feature_names, loaded_sparse_matrix = save_and_load_data(vectorizer, all_sparse_matrix, filename_feature,\n",
    "                                                                   filename_matrix)\n",
    "    total_word_occurrences = calculate_word_occurrences(loaded_feature_names, loaded_sparse_matrix)\n",
    "\n",
    "    result_list = []\n",
    "\n",
    "    for article_id, article in article_db.items():\n",
    "        title = article.get('Titre', '')\n",
    "        description = article.get('Description / Résumé', '')\n",
    "        content = article.get('Contenu', '')\n",
    "        combined_text = f\"{title} {description} {content}\"\n",
    "        text_without_punctuation = re.sub(r'[^\\w\\s]', '', combined_text)\n",
    "        text_lower = text_without_punctuation.lower()\n",
    "        text_without_numbers = re.sub(r'\\d+', '', text_lower)\n",
    "        words = text_without_numbers.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "        stemmed_words = [stemmer.stemWord(word) for word in filtered_words]\n",
    "        processed_text = ' '.join(stemmed_words)\n",
    "        word_occurrences_dict = {word: total_word_occurrences[word] for word in processed_text.split()}\n",
    "\n",
    "        category = article.get('Catégorie', '')\n",
    "\n",
    "\n",
    "\n",
    "        result_list.append({'document': article_id, 'word_occurrences': word_occurrences_dict,\n",
    "                            'catégorie': category })\n",
    "\n",
    "    result_dataset = pd.DataFrame(result_list)\n",
    "    return result_dataset\n",
    "\n",
    "# Language settings\n",
    "lang_french = \"french\"\n",
    "lang_english = \"english\"\n",
    "\n",
    "# Snowball stemmers and stop words\n",
    "stemmer_french = snowballstemmer.stemmer(lang_french)\n",
    "stopwords_french = get_stop_words(lang_french)\n",
    "\n",
    "stemmer_english = snowballstemmer.stemmer(lang_english)\n",
    "stopwords_english = get_stop_words(lang_english)\n",
    "\n",
    "# Open the shelve file for reading\n",
    "article_db = shelve.open('./benchmark/defi_db', 'r')\n",
    "\n",
    "# Separate articles by language\n",
    "article_db_french, article_db_english = separate_articles_by_language(article_db)\n",
    "\n",
    "# Close the shelve file when done\n",
    "article_db.close()\n",
    "\n",
    "# Use CountVectorizer directly on processed texts for each language\n",
    "vectorizer_french = CountVectorizer(stop_words=stopwords_french)\n",
    "all_processed_texts_french = process_articles(article_db_french, stopwords_french, stemmer_french)\n",
    "all_sparse_matrix_french = vectorizer_french.fit_transform(all_processed_texts_french)\n",
    "\n",
    "vectorizer_english = CountVectorizer(stop_words=stopwords_english)\n",
    "all_processed_texts_english = process_articles(article_db_english, stopwords_english, stemmer_english)\n",
    "all_sparse_matrix_english = vectorizer_english.fit_transform(all_processed_texts_english)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset for French\n",
    "defi_french = create_dataset(article_db_french, stopwords_french, stemmer_french, vectorizer_french,\n",
    "                                'feature_names_defi_french.joblib', 'sparse_matrix_defi_french.joblib', lang_french)\n",
    "\n",
    "# Create dataset for English\n",
    "defi_english = create_dataset(article_db_english, stopwords_english, stemmer_english, vectorizer_english,\n",
    "                                 'feature_names_defi_english.joblib', 'sparse_matrix_defi_english.joblib', lang_english)\n",
    "\n",
    "# Display the datasets\n",
    "print(\"\\nDataset for French:\")\n",
    "print(defi_french.head())\n",
    "\n",
    "print(\"\\nDataset for English:\")\n",
    "print(defi_english.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c6a42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>word_occurrences</th>\n",
       "      <th>catégorie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>527ba1db806cfd245d90526c5a7b9342</td>\n",
       "      <td>{'b': 0, 'london': 27, 'lion': 103, 'do': 44, ...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150854b0beccb3ee7d55fad80f92e58b</td>\n",
       "      <td>{'b': 0, 'french': 110, 'football': 1626, 'pre...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e9ad60a84487a9e79cc3e95fc1d60f95</td>\n",
       "      <td>{'b': 0, 'patisser': 27, 'valer': 113, 'four':...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>395be8ab43a1833438a972ac5ecd0f48</td>\n",
       "      <td>{'b': 0, 'nas': 566, 'spacex': 232, 'launch': ...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4722474dddcd6a982556ec48b8b07ab3</td>\n",
       "      <td>{'b': 0, 'russian': 17, 'influx': 1, 'driv': 3...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69411</th>\n",
       "      <td>ce384ee0886519501c7d197fb26dad3c</td>\n",
       "      <td>{'b': 0, 'anod': 6, 'catalyseur': 10, 'bio': 7...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69412</th>\n",
       "      <td>1fcc25ec3f38cac13d439395e3fe83eb</td>\n",
       "      <td>{'b': 0, 'video': 1800, 'assemble': 2850, 'nat...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69413</th>\n",
       "      <td>5fa23821f22a069f1cac6154070d428d</td>\n",
       "      <td>{'b': 0, 'equip': 3966, 'franc': 18198, 'attaq...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69414</th>\n",
       "      <td>d09ab27a292d00d6ed34e1e24dc8079c</td>\n",
       "      <td>{'b': 0, 'attend': 607, 'fin': 3321, 'weekend'...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69415</th>\n",
       "      <td>af9a7b477d7255002f1da8dd04cc649f</td>\n",
       "      <td>{'b': 0, 'judo': 101, 'dout': 339, 'rin': 38, ...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69416 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               document  \\\n",
       "0      527ba1db806cfd245d90526c5a7b9342   \n",
       "1      150854b0beccb3ee7d55fad80f92e58b   \n",
       "2      e9ad60a84487a9e79cc3e95fc1d60f95   \n",
       "3      395be8ab43a1833438a972ac5ecd0f48   \n",
       "4      4722474dddcd6a982556ec48b8b07ab3   \n",
       "...                                 ...   \n",
       "69411  ce384ee0886519501c7d197fb26dad3c   \n",
       "69412  1fcc25ec3f38cac13d439395e3fe83eb   \n",
       "69413  5fa23821f22a069f1cac6154070d428d   \n",
       "69414  d09ab27a292d00d6ed34e1e24dc8079c   \n",
       "69415  af9a7b477d7255002f1da8dd04cc649f   \n",
       "\n",
       "                                        word_occurrences catégorie  \n",
       "0      {'b': 0, 'london': 27, 'lion': 103, 'do': 44, ...         ?  \n",
       "1      {'b': 0, 'french': 110, 'football': 1626, 'pre...         ?  \n",
       "2      {'b': 0, 'patisser': 27, 'valer': 113, 'four':...         ?  \n",
       "3      {'b': 0, 'nas': 566, 'spacex': 232, 'launch': ...         ?  \n",
       "4      {'b': 0, 'russian': 17, 'influx': 1, 'driv': 3...         ?  \n",
       "...                                                  ...       ...  \n",
       "69411  {'b': 0, 'anod': 6, 'catalyseur': 10, 'bio': 7...         ?  \n",
       "69412  {'b': 0, 'video': 1800, 'assemble': 2850, 'nat...         ?  \n",
       "69413  {'b': 0, 'equip': 3966, 'franc': 18198, 'attaq...         ?  \n",
       "69414  {'b': 0, 'attend': 607, 'fin': 3321, 'weekend'...         ?  \n",
       "69415  {'b': 0, 'judo': 101, 'dout': 339, 'rin': 38, ...         ?  \n",
       "\n",
       "[69416 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_french"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973e6c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>word_occurrences</th>\n",
       "      <th>catégorie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a5319b640496be985be9e46141a44d26</td>\n",
       "      <td>{'b': 0, 'true': 155, 'coronavirus': 3840, 'de...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>733691e2b8bb30046fcb3b3870acdda9</td>\n",
       "      <td>{'b': 0, 'unearth': 42, 'maya': 28, 'civil': 2...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aae4c53ffd63ab90fafe911020a39411</td>\n",
       "      <td>{'b': 0, 't': 0, 'row': 361, 'price': 1403, 'r...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd90e1776424ffb662d7502846824612</td>\n",
       "      <td>{'b': 0, 'new': 7788, 'voluntari': 31, 'mobili...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>729d9fa185f1ab06151e847d1e3aacea</td>\n",
       "      <td>{'b': 0, 'bmw': 18, 'say': 9018, 'will': 7135,...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58248</th>\n",
       "      <td>0bda59e8dc68f3a997ae5ff2888aff35</td>\n",
       "      <td>{'b': 0, 'minecraft': 10, 'legend': 237, 'terr...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58249</th>\n",
       "      <td>182e9e428854d4823557db43b26a4fa6</td>\n",
       "      <td>{'b': 0, 'bfm': 339, 'patrimoin': 10, 'h': 0, ...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58250</th>\n",
       "      <td>084da97559c8a0d17268573fe515fffe</td>\n",
       "      <td>{'b': 0, 'l': 0, 'integral': 3, 'de': 2448, 'g...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58251</th>\n",
       "      <td>61d784b9e5f9de2319c013a6e64adef8</td>\n",
       "      <td>{'b': 0, 'contenus': 8, 'pro': 226, 'nazi': 67...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58252</th>\n",
       "      <td>351de2564ca957918f49713afb70387f</td>\n",
       "      <td>{'b': 0, 'black': 1231, 'friday': 1100, 'doubl...</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               document  \\\n",
       "0      a5319b640496be985be9e46141a44d26   \n",
       "1      733691e2b8bb30046fcb3b3870acdda9   \n",
       "2      aae4c53ffd63ab90fafe911020a39411   \n",
       "3      bd90e1776424ffb662d7502846824612   \n",
       "4      729d9fa185f1ab06151e847d1e3aacea   \n",
       "...                                 ...   \n",
       "58248  0bda59e8dc68f3a997ae5ff2888aff35   \n",
       "58249  182e9e428854d4823557db43b26a4fa6   \n",
       "58250  084da97559c8a0d17268573fe515fffe   \n",
       "58251  61d784b9e5f9de2319c013a6e64adef8   \n",
       "58252  351de2564ca957918f49713afb70387f   \n",
       "\n",
       "                                        word_occurrences catégorie  \n",
       "0      {'b': 0, 'true': 155, 'coronavirus': 3840, 'de...         ?  \n",
       "1      {'b': 0, 'unearth': 42, 'maya': 28, 'civil': 2...         ?  \n",
       "2      {'b': 0, 't': 0, 'row': 361, 'price': 1403, 'r...         ?  \n",
       "3      {'b': 0, 'new': 7788, 'voluntari': 31, 'mobili...         ?  \n",
       "4      {'b': 0, 'bmw': 18, 'say': 9018, 'will': 7135,...         ?  \n",
       "...                                                  ...       ...  \n",
       "58248  {'b': 0, 'minecraft': 10, 'legend': 237, 'terr...         ?  \n",
       "58249  {'b': 0, 'bfm': 339, 'patrimoin': 10, 'h': 0, ...         ?  \n",
       "58250  {'b': 0, 'l': 0, 'integral': 3, 'de': 2448, 'g...         ?  \n",
       "58251  {'b': 0, 'contenus': 8, 'pro': 226, 'nazi': 67...         ?  \n",
       "58252  {'b': 0, 'black': 1231, 'friday': 1100, 'doubl...         ?  \n",
       "\n",
       "[58253 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defi_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af55e490",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.0 GiB for an array with shape (55532, 43428) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Diviser le dataset en ensembles d'entraînement et de test\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Initialiser les classifieurs\u001b[39;00m\n\u001b[0;32m     27\u001b[0m classifiers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk-NN\u001b[39m\u001b[38;5;124m'\u001b[39m: KNeighborsClassifier(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     34\u001b[0m }\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2640\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2638\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m-> 2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2641\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2643\u001b[0m     )\n\u001b[0;32m   2644\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2642\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   2636\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m   2638\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[0;32m   2640\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   2641\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 2642\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[0;32m   2643\u001b[0m     )\n\u001b[0;32m   2644\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:355\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pandas_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _array_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:184\u001b[0m, in \u001b[0;36m_array_indexing\u001b[1;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    183\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array[key] \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.0 GiB for an array with shape (55532, 43428) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset (assurez-vous que dataset_french est correctement défini)\n",
    "df = defi_french\n",
    "\n",
    "# Diviser le dataset en features (X) et la target (y)\n",
    "X = df['word_occurrences']\n",
    "y = df['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser les classifieurs\n",
    "classifiers = {\n",
    "    'k-NN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Comparaison des classifieurs avec validation croisée\n",
    "results = {'Classifier': [], 'Accuracy': []}\n",
    "\n",
    "# Définir une stratégie de validation croisée (StratifiedKFold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Entraîner le classifieur sur l'ensemble d'entraînement\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluer le classifieur sur l'ensemble de test\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results['Classifier'].append(clf_name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualisation des résultats avec des barres\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['Classifier'], results_df['Accuracy'], color='blue', alpha=0.7, label='Accuracy')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f2926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset (assurez-vous que dataset_french est correctement défini)\n",
    "df = dataset_english\n",
    "\n",
    "# Diviser le dataset en features (X) et la target (y)\n",
    "X = df['word_occurrences']\n",
    "y = df['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser les classifieurs\n",
    "classifiers = {\n",
    "    'k-NN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Comparaison des classifieurs avec validation croisée\n",
    "results = {'Classifier': [], 'Accuracy': []}\n",
    "\n",
    "# Définir une stratégie de validation croisée (StratifiedKFold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Entraîner le classifieur sur l'ensemble d'entraînement\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluer le classifieur sur l'ensemble de test\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results['Classifier'].append(clf_name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Visualisation des résultats avec des barres\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results_df['Classifier'], results_df['Accuracy'], color='blue', alpha=0.7, label='Accuracy')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Comparison of Classifiers')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ddb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset (assurez-vous que dataset_french est correctement défini)\n",
    "df = dataset_french\n",
    "\n",
    "# Diviser le dataset en features (X) et la target (y)\n",
    "X = df['word_occurrences']\n",
    "y = df['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser les classifieurs\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'LogReg': LogisticRegression(random_state=42),\n",
    "    'Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Comparaison des classifieurs\n",
    "results = {'Classifier': [], 'Accuracy': [], 'Precision (micro)': [], 'Recall (micro)': [], 'AUC (micro)': [],\n",
    "           'Precision (macro)': [], 'Recall (macro)': [], 'AUC (macro)': []}\n",
    "\n",
    "# Définir une stratégie de validation croisée (StratifiedKFold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Entraîner le classifieur sur l'ensemble d'entraînement\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédire les catégories pour l'ensemble de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "    roc_auc_micro = roc_auc_score(label_binarize(y_test, classes=clf.classes_), label_binarize(y_pred, classes=clf.classes_), average='micro')\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    roc_auc_macro = roc_auc_score(label_binarize(y_test, classes=clf.classes_), label_binarize(y_pred, classes=clf.classes_), average='macro')\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results['Classifier'].append(clf_name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Precision (micro)'].append(precision_micro)\n",
    "    results['Recall (micro)'].append(recall_micro)\n",
    "    results['AUC (micro)'].append(roc_auc_micro)\n",
    "    results['Precision (macro)'].append(precision_macro)\n",
    "    results['Recall (macro)'].append(recall_macro)\n",
    "    results['AUC (macro)'].append(roc_auc_macro)\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualisation des résultats avec des barres\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.bar(results_df['Classifier'], results_df['Accuracy'], color='blue', alpha=0.7)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Precision (micro)\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.bar(results_df['Classifier'], results_df['Precision (micro)'], color='green', alpha=0.7)\n",
    "plt.title('Precision (micro)')\n",
    "\n",
    "# Recall (micro)\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.bar(results_df['Classifier'], results_df['Recall (micro)'], color='orange', alpha=0.7)\n",
    "plt.title('Recall (micro)')\n",
    "\n",
    "# AUC (micro)\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.bar(results_df['Classifier'], results_df['AUC (micro)'], color='red', alpha=0.7)\n",
    "plt.title('AUC (micro)')\n",
    "\n",
    "# Precision (macro)\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.bar(results_df['Classifier'], results_df['Precision (macro)'], color='purple', alpha=0.7)\n",
    "plt.title('Precision (macro)')\n",
    "\n",
    "# Recall (macro)\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.bar(results_df['Classifier'], results_df['Recall (macro)'], color='brown', alpha=0.7)\n",
    "plt.title('Recall (macro)')\n",
    "\n",
    "# AUC (macro)\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.bar(results_df['Classifier'], results_df['AUC (macro)'], color='pink', alpha=0.7)\n",
    "plt.title('AUC (macro)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aa9bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger le dataset (assurez-vous que dataset_french est correctement défini)\n",
    "df = dataset_english\n",
    "\n",
    "# Diviser le dataset en features (X) et la target (y)\n",
    "X = df['word_occurrences']\n",
    "y = df['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(X)\n",
    "\n",
    "# Diviser le dataset en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser les classifieurs\n",
    "classifiers = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
    "    'LogReg': LogisticRegression(random_state=42),\n",
    "    'Bayes': GaussianNB(),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'Neural': MLPClassifier(hidden_layer_sizes=(100,), max_iter=500, random_state=42),\n",
    "    'RF': RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Comparaison des classifieurs\n",
    "results = {'Classifier': [], 'Accuracy': [], 'Precision (micro)': [], 'Recall (micro)': [], 'AUC (micro)': [],\n",
    "           'Precision (macro)': [], 'Recall (macro)': [], 'AUC (macro)': []}\n",
    "\n",
    "# Définir une stratégie de validation croisée (StratifiedKFold)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Entraîner le classifieur sur l'ensemble d'entraînement\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Prédire les catégories pour l'ensemble de test\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_micro = precision_score(y_test, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_test, y_pred, average='micro')\n",
    "    roc_auc_micro = roc_auc_score(label_binarize(y_test, classes=clf.classes_), label_binarize(y_pred, classes=clf.classes_), average='micro')\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro')\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro')\n",
    "    roc_auc_macro = roc_auc_score(label_binarize(y_test, classes=clf.classes_), label_binarize(y_pred, classes=clf.classes_), average='macro')\n",
    "    \n",
    "    # Stocker les résultats\n",
    "    results['Classifier'].append(clf_name)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Precision (micro)'].append(precision_micro)\n",
    "    results['Recall (micro)'].append(recall_micro)\n",
    "    results['AUC (micro)'].append(roc_auc_micro)\n",
    "    results['Precision (macro)'].append(precision_macro)\n",
    "    results['Recall (macro)'].append(recall_macro)\n",
    "    results['AUC (macro)'].append(roc_auc_macro)\n",
    "\n",
    "# Afficher les résultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Visualisation des résultats avec des barres\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.bar(results_df['Classifier'], results_df['Accuracy'], color='blue', alpha=0.7)\n",
    "plt.title('Accuracy')\n",
    "\n",
    "# Precision (micro)\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.bar(results_df['Classifier'], results_df['Precision (micro)'], color='green', alpha=0.7)\n",
    "plt.title('Precision (micro)')\n",
    "\n",
    "# Recall (micro)\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.bar(results_df['Classifier'], results_df['Recall (micro)'], color='orange', alpha=0.7)\n",
    "plt.title('Recall (micro)')\n",
    "\n",
    "# AUC (micro)\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.bar(results_df['Classifier'], results_df['AUC (micro)'], color='red', alpha=0.7)\n",
    "plt.title('AUC (micro)')\n",
    "\n",
    "# Precision (macro)\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.bar(results_df['Classifier'], results_df['Precision (macro)'], color='purple', alpha=0.7)\n",
    "plt.title('Precision (macro)')\n",
    "\n",
    "# Recall (macro)\n",
    "plt.subplot(3, 3, 6)\n",
    "plt.bar(results_df['Classifier'], results_df['Recall (macro)'], color='brown', alpha=0.7)\n",
    "plt.title('Recall (macro)')\n",
    "\n",
    "# AUC (macro)\n",
    "plt.subplot(3, 3, 7)\n",
    "plt.bar(results_df['Classifier'], results_df['AUC (macro)'], color='pink', alpha=0.7)\n",
    "plt.title('AUC (macro)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e57b942",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.5 GiB for an array with shape (69416, 43428) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Convertir les occurrences de mots en vecteurs\u001b[39;00m\n\u001b[0;32m     19\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m DictVectorizer(sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m X_existing \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_existing)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Entraîner le meilleur classifieur sur l'ensemble de données existant\u001b[39;00m\n\u001b[0;32m     23\u001b[0m best_classifier\u001b[38;5;241m.\u001b[39mfit(X_existing, y_existing)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:313\u001b[0m, in \u001b[0;36mDictVectorizer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \n\u001b[0;32m    293\u001b[0m \u001b[38;5;124;03m    Like fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m        Feature vectors; always 2-d.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(X, fitting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:281\u001b[0m, in \u001b[0;36mDictVectorizer._transform\u001b[1;34m(self, X, fitting)\u001b[0m\n\u001b[0;32m    279\u001b[0m     result_matrix\u001b[38;5;241m.\u001b[39msort_indices()\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 281\u001b[0m     result_matrix \u001b[38;5;241m=\u001b[39m result_matrix\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fitting:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_ \u001b[38;5;241m=\u001b[39m feature_names\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1050\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1051\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1298\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 22.5 GiB for an array with shape (69416, 43428) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Supposons que vous ayez une nouvelle structure item-RSS appelée new_data\n",
    "# new_data doit contenir un champ 'word_occurrences' similaire à votre ensemble d'entraînement\n",
    "\n",
    "# Charger le meilleur classifieur RandomForest (assurez-vous de remplacer cela par votre meilleur classifieur)\n",
    "best_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Remplacez cela par votre meilleur classifieur\n",
    "\n",
    "# Charger l'ensemble de données existant\n",
    "existing_data = defi_french\n",
    "\n",
    "# Diviser l'ensemble de données existant en features (X) et la target (y)\n",
    "X_existing = existing_data['word_occurrences']\n",
    "y_existing = existing_data['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X_existing = vectorizer.fit_transform(X_existing)\n",
    "\n",
    "# Entraîner le meilleur classifieur sur l'ensemble de données existant\n",
    "best_classifier.fit(X_existing, y_existing)\n",
    "\n",
    "# Supposons que new_data contienne un champ 'word_occurrences' similaire à votre ensemble d'entraînement\n",
    "X_new = dataset_french['word_occurrences']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs (utilisez le même vectorizer)\n",
    "X_new = vectorizer.transform(X_new)\n",
    "\n",
    "# Prédire les catégories pour les nouvelles données\n",
    "y_pred_new = best_classifier.predict(X_new)\n",
    "\n",
    "# Obtenir les probabilités associées à chaque prédiction\n",
    "probs_new = best_classifier.predict_proba(X_new)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Predicted:\", list(y_pred_new))\n",
    "print(\"Label Probability Prediction Order:\", best_classifier.classes_)\n",
    "print(\"Probs:\", probs_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18da22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Supposons que vous ayez une nouvelle structure item-RSS appelée new_data\n",
    "# new_data doit contenir un champ 'word_occurrences' similaire à votre ensemble d'entraînement\n",
    "\n",
    "# Charger le meilleur classifieur RandomForest (assurez-vous de remplacer cela par votre meilleur classifieur)\n",
    "best_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # Remplacez cela par votre meilleur classifieur\n",
    "\n",
    "# Charger l'ensemble de données existant\n",
    "existing_data = dataset_french\n",
    "\n",
    "# Diviser l'ensemble de données existant en features (X) et la target (y)\n",
    "X_existing = existing_data['word_occurrences']\n",
    "y_existing = existing_data['catégorie']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X_existing = vectorizer.fit_transform(X_existing)\n",
    "\n",
    "# Entraîner le meilleur classifieur sur l'ensemble de données existant\n",
    "best_classifier.fit(X_existing, y_existing)\n",
    "\n",
    "# Supposons que new_data contienne un champ 'word_occurrences' similaire à votre ensemble d'entraînement\n",
    "X_new = defi_french['word_occurrences']\n",
    "\n",
    "# Convertir les occurrences de mots en vecteurs (utilisez le même vectorizer)\n",
    "X_new = vectorizer.transform(X_new)\n",
    "\n",
    "# Prédire les catégories pour les nouvelles données (seulement pour le premier article)\n",
    "y_pred_new = best_classifier.predict(X_new)\n",
    "\n",
    "# Obtenir les probabilités associées à chaque prédiction (seulement pour le premier article)\n",
    "probs_new = best_classifier.predict_proba(X_new)\n",
    "\n",
    "# Créer un DataFrame avec les résultats pour le premier article\n",
    "results_df = pd.DataFrame(probs_new, columns=best_classifier.classes_)\n",
    "results_df.insert(0, 'Classe Prédite', y_pred_new)\n",
    "\n",
    "# Afficher le DataFrame\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83491b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f06cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba2776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a2428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
